Title: Bericht MLUGS Treffen im September 2019
Date: 2019-09-17 18:30

## Protokoll

### Vorstellungsrunde

- Andreas, Software-Entwickler, AX-Semantics
- Uwe, Laser-Kommunikation. ST2C
- Xiake, ITK Engineering
- Kexin, Machine-Learning-Eng., Bosch
- Nela, Daimler
- Ioana, Bosch
- Michael, Software-Engineer, AX-Semantics
- Christian, baut Anlagen zur Batteriefertigung, Daimler
- Tin, wiss. Mitarbeiter, Frauenhofer
- Thomas, Data-Scientist
- Kai, Anfänger in ML
- Sasha, Consultant, ADWEKO
- Jens, Data-Scientist, AKKA
- Sebastian, Software-Engineer, IBM
- Nicola, Journalistin, will Insekten zählen


### Nicola: Vorstellung Insekten-Counter-Projekt mit Maschine Learning

- <https://ecocurious.de>
- <https://www.meetup.com/Ecocurious-deine-Umwelt-Natur-und-Technik-Community/>

- Methode 1: optisches Mikrofon - Infrarot-Dioden auf Licht-Sensoren -> akkustik
- Methode 2: mit Kamera fotografieren (raspi + picam)

- suchen Unterstützung bei der Hardwareentwicklung und beim Klassifizierung
- verwendet gnuradio zur Auswertung

- aktueller Stand: methoden finden Daten zu sammeln; danach klassifiezieren wieviele/welche Insekten

- Bienen zählen: <https://www.apic.ai>
- event-camera: <http://rpg.ifi.uzh.ch/research_dvs.html>
- vielleicht in der Wilhelma im Tropenhaus Fotos machen
- "zeitmultiplexing Spektroskopie"


### Sebastian: regelbasierte vs. ML-basierte Informationsextraktion

- rule-based am Beispiel von AQL: vorteile der RDMBS-Erfahrungen ausnutzen
- Regeln schreiben kann sehr viel besser sein als (nur) Trainingsdaten für ein NN zu annotieren
- 80%-NNs sind noch nicht produktionsreif!
- es gibt Probleme, die man immernoch am besten mit rule-based-systemen löst


### Andreas: Encoder-Decoder Attention plots

#### recap

- <https://github.com/mfa/talks/blob/master/20190521--morphological-inflection-using-encoder-decoder-networks/slides.md>

#### insights about attention with a simple example

- reimplementation of <https://joeynmt.readthedocs.io/en/latest/tutorial.html> with allennlp
- code: <https://github.com/mfa/allennlp-reverse-seq2seq>
- <https://madflex.de/posts/allennlp--reverse-sequence-example/>
- uses general/bilinear attention: <https://arxiv.org/pdf/1508.04025.pdf>
- model code is a copy of allennlp version with a patch applied, that adds attention vectors to output
- <https://madflex.de/posts/allennlp--sequence-to-sequence-attention-plots/>


### next

- date: 2019-10-15
- talks:
  - Tin: machine learning and security
  - Andreas: digit recognition
